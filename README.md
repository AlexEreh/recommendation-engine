# Рекомендательные алгоритмы

## Используемые библиотеки

- [Polars](https://pola.rs/)
- [Pandas](https://pandas.pydata.org/) (для работы с библиотеками, которые не поддерживают Polars)
- [NumPy](https://numpy.org/)
- [SciPy](https://scipy.org/)
- [SciKit Learn](https://scikit-learn.org/stable/)

# Использованный датасет

- [Сабсет датасета Full MovieLens Dataset](https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset) - содержит
  45000 фильмов, вышедших до 2017 года. Часть данных была получена с TMDB Open API (ключевые слова и титры)

## Требования к запуску

- Гигов 15-20 оперативной памяти,
  либо достаточно большой своп раздел,
  либо большое желание использовать жупитер нот э бок в Google Colaboratory

## Рассмотренные алгоритмы

### Простейший (на базе функции взвешенного рейтинга)

Даёт общие рекомендации каждому пользователю, на основании популярности фильма, либо на его жанре.
Основная идея в том, что фильмы которые более популярны и признаны критиками будут иметь
большую вероятность быть высоко оценёнными среднеми зрителями.

Примером является топ фильмов IMDB.

В процессе расчёта используется смледующая функция взвешенного рейтинга:

![Формула взвешенного рейтинга](docs/weighted_rating_equation.png)

Алгоритм находится [здесь](content_simple.py)

![Результат](docs/simple_result.png)

### По содержимому

Они предлагают похожие фильмы на основе конкретного фильма.
Для создания таких рекомендаций система использует метаданные, такие как жанр, режиссер, описание, актеры и т.д.

Общая идея этих рекомендательных систем заключается в том,
что если человеку нравится определенный фильм, то ему также понравится и похожий на него фильм.
И чтобы порекомендовать его, система использует метаданные о фильмах, просмотренных пользователем в прошлом.

Хорошим примером может послужить YouTube, где на основе вашей истории он предлагает вам новые видео,
которые вы могли бы посмотреть.

#### На базе анализа выделения ключевых слов с помощью векторизатора TF-IDF

Внутри датафрейма метаданных фильмов есть столбец overview, в котором содержится описание фильма.

Сперва допустим, что у нас нет никаких тегов для категоризации фильмов - только описание.

Данная задача - это задача `обработки естественного языка` **(NLP)**.
Следовательно, нам необходимо извлечь некоторые характеристики из приведенных выше текстовых данных,
прежде чем вычислять сходство и/или несходство между ними.
Невозможно вычислить сходство между двумя обзорами в их необработанном виде.
Для этого необходимо вычислить векторы слов каждого обзора или документа, как это будет называться в дальнейшем.

`Векторы слов` - это векторное представление слов в документе.
Векторы несут в себе семантическое значение. Например, слова man & king будут иметь векторные представления,
близкие друг к другу, а слова man & woman - далекие друг от друга.

Мы рассчитаем векторы `Term Frequency-Inverse Document Frequency (TF-IDF)` для каждого документа.
Это даст нам матрицу, в которой каждый столбец представляет слово из словаря обзора
(все слова, которые встречаются хотя бы в одном документе), а каждый столбец представляет фильм, как и раньше.

По сути, показатель TF-IDF - это частота встречаемости слова в документе, уменьшенная на количество документов,
в которых оно встречается. Это делается для того, чтобы уменьшить важность слов,
которые часто встречаются в сюжетных обзорах, и, следовательно, их значимость при вычислении итогового балла сходства.

Мы будем использовать `косинусоидальное сходство` для вычисления числовой величины,
обозначающей сходство между двумя фильмами.
Мы используем косинусоидальное сходство, поскольку оно не зависит от величины,
а также относительно легко и быстро вычисляется
(особенно при использовании в сочетании с показателями TF-IDF).

Математически оно определяется следующим образом:

![Косинусоидальное сходство](docs/cosine_similarity.png)

Поскольку мы использовали векторизатор TF-IDF,
вычисление точечного произведения между каждым вектором напрямую даст вам оценку косинусного сходства.
Поэтому мы будем использовать функцию sklearn `linear_kernel()` вместо `cosine_similarities()`, так как она быстрее.

Алгоритм находится [здесь](content_tfidf.py)

**Результат для фильма 'The Dark Knight Rises'**

![Результат](docs/tfidf_result.png)

#### На базе анализа ключевых слов датасета

Качество рекомендателя повысится, если использовать более качественные метаданные и
улавливать больше мелких деталей.
Именно поэтому нам нужны данные о [съемочной группе](dataset/credits.csv), [ключевые слова](dataset/keywords.csv)
и другая информация из метаданных фильмов.

Алгоритм находится [здесь](content_keywords.py)

**Результат для фильма 'The Dark Knight Rises'**

![Результат](docs/keywords_result.png)